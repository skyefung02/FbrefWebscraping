{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the pipeline to perform the necessary feature transformations prior to model fitting. The necessary changes are listed below. \n",
    "\n",
    "* Removing observations where 'Minutes Played' == 0\n",
    "* Include 'Venue' as important feature\n",
    "* Include 'Minutes Played' as important feature\n",
    "* One-hot encode 'Position', and then further filter into 'Defenders', 'Midfielders' and 'Attackers'\n",
    "* Create 'Season' feature using 'kickoff_time', as this feature is necessary to calculate 'Designated Penalty Takers' \n",
    "* Use 'Penalties Attempted' to calculate 'Designated Penalty Takers' \n",
    "* Include 'Shots on Target' as important feature \n",
    "* Include 'npxG' as important feature\n",
    "* Include 'Penalty Area Touches' as important feature\n",
    "* Compute 'Rolling xG' which will replace 'npxG', then drop 'npxG'\n",
    "* Calculate 'Team Rolling xG Matchups' by first using team data to first calculate 'Team Rolling xG' and 'Team Rolling xGA'. We can then calculate 'Team Rolling xG Difference', which allows us to calculate 'Team Rolling xG Matchups'\n",
    "* Calculate 'Rolling Shots on Target' to replace 'Shots on Target', then drop 'Shots on Target'\n",
    "* Calculate 'Rolling Penalty Area Touches' to replace 'Penalty Area Touches', then drop 'Penalty Area Touches'\n",
    "\n",
    "This should leave the final dataframe with the following features - Venue, Designated Penalty Taker, Rolling Shots on Target, Rolling xG, Rolling Penalty Area Touches, Team Rolling xG Matchup, Defenders, Midfielders, Attackers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant dataframes from source\n",
    "att_train = pd.read_csv('att_explore_original.csv', index_col = 0)\n",
    "att_test = pd.read_csv('att_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to select specific columns\n",
    "def select_columns(dataframe):\n",
    "    columns = ['Player ID', 'Team', 'Opponent', 'Venue', 'Goals', 'Minutes Played', 'Position', 'kickoff_time', 'Penalties Attempted', \n",
    "               'Shots on Target', 'npxG', 'Penalty Area Touches'] \n",
    "    return dataframe[columns].copy()\n",
    "\n",
    "#transformer to drop rows with empty 'Position'\n",
    "class DropEmptyPositions(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[X['Position'] != '0']\n",
    "\n",
    "#transformer for one-hot encoding and creating position categories\n",
    "class PositionEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #one hot encode positions\n",
    "        positions_encode = X['Position'].str.get_dummies(sep=',')\n",
    "        \n",
    "        #create new position categories\n",
    "        positions_encode['Defender'] = positions_encode[['RB', 'LB', 'CB']].any(axis=1).astype(int)\n",
    "        positions_encode['Midfielder'] = positions_encode[['DM', 'CM', 'LM', 'RM', 'AM']].any(axis=1).astype(int)\n",
    "        positions_encode['Attacker'] = positions_encode[['LW', 'RW', 'FW']].any(axis=1).astype(int)\n",
    "        \n",
    "        # Drop original position columns\n",
    "        positions_encode = positions_encode.drop(columns=['RB', 'LB', 'CB', 'DM', 'CM', 'LM', 'RM', 'LW', 'RW', 'AM', 'FW', 'WB'], \n",
    "                                                 errors='ignore')\n",
    "        \n",
    "        X = X.drop('Position', axis = 1)\n",
    "        \n",
    "        return pd.concat([X.reset_index(drop=True), positions_encode.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#transformer for determining the season\n",
    "class SeasonDeterminer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert 'kickoff_time' to datetime if not already done\n",
    "        X['kickoff_time'] = pd.to_datetime(X['kickoff_time'])\n",
    "        \n",
    "        # Function to determine the season\n",
    "        def determine_season(kickoff_time):\n",
    "            month = kickoff_time.month\n",
    "            year = kickoff_time.year\n",
    "            if month >= 8:  # August to December\n",
    "                return f'{year}-{year + 1}'  # Current year to next year\n",
    "            else:  # January to July\n",
    "                return f'{year - 1}-{year}'  # Previous year to current year\n",
    "        \n",
    "        # Apply the function to create the 'Season' column\n",
    "        X['Season'] = X['kickoff_time'].apply(determine_season)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "#transformer for calculating 'Designated Penalty Taker'\n",
    "class DesigPenTaker(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #group observations by player ID and penalties attempted \n",
    "        pen_group = X.groupby('Player ID', as_index = False)['Penalties Attempted'].sum()\n",
    "\n",
    "        #remove obs with 0 penalties attempted \n",
    "        pen_group = pen_group[pen_group['Penalties Attempted'] > 0 ]\n",
    "\n",
    "        #create new dataframe which has 'kickoff_time', 'Season', 'penalties attempted' and 'team in it \n",
    "        team_pens = X[['kickoff_time', 'Season', 'Penalties Attempted', 'Team']].copy()\n",
    "\n",
    "        #now we group by team and season to compute how many penalties were taken by each team in each season\n",
    "        team_pens_summary = team_pens.groupby(['Season', 'Team'], as_index=False)['Penalties Attempted'].sum()\n",
    "        team_pens_summary.rename(columns={'Penalties Attempted': 'Team Penalties'}, inplace=True)\n",
    "\n",
    "        #create empty dataframe\n",
    "        pen_prop = pd.DataFrame()\n",
    "\n",
    "        #loop through to get the Player ID and Penalties Attempted for each team in each season, filtering so that we only \n",
    "        # include observations with at least 1 penalty taken \n",
    "        for index, row in team_pens_summary.iterrows():\n",
    "            team = row['Team']\n",
    "            season = row['Season']\n",
    "    \n",
    "            filtered = X[(X['Season'] == season) & (X['Team'] == team) & (X['Penalties Attempted'] > 0)][['Player ID', 'Penalties Attempted']]\n",
    "            filtered['Team'] = team\n",
    "            filtered['Season'] = season\n",
    "            pen_prop = pd.concat([pen_prop, filtered], ignore_index= True)\n",
    "\n",
    "        #adding a new column into pen_prop called 'Team Penalties' which merges the relevant information from team_pens_summary\n",
    "        pen_prop = pen_prop.merge(team_pens_summary, on=['Team', 'Season'], how='left')\n",
    "\n",
    "        #we now merge rows that have the same player ID, team and season together. For the rows that satisfy this, we sum the \n",
    "        # penalties attempted to reflect the number of penalties a particular player ID took in a given season \n",
    "        merged_penprop = pen_prop.groupby(['Team', 'Season', 'Player ID'], as_index=False).agg({\n",
    "            'Penalties Attempted': 'sum',\n",
    "            'Team Penalties': 'first'  \n",
    "        })\n",
    "        merged_penprop = merged_penprop.sort_values(by='Player ID')\n",
    "\n",
    "        #adding new column called Proportion of Team Penalties Taken\n",
    "        merged_penprop['Proportion of Team Penalties Taken'] = (merged_penprop['Penalties Attempted'] / merged_penprop['Team Penalties'])\n",
    "\n",
    "        #final dataframe which merges the rows based on Player ID. Each row now corresponds to one unique player ID, the penalties\n",
    "        # attempted and team penalties columns are now summed. The proportion is then recalculated \n",
    "        penprop_summary = merged_penprop.groupby('Player ID').agg(\n",
    "            Penalties_Attempted=('Penalties Attempted', 'sum'),\n",
    "            Team_Penalties=('Team Penalties', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        penprop_summary['Proportion of Team Penalties Taken'] = (\n",
    "        penprop_summary['Penalties_Attempted'] / penprop_summary['Team_Penalties'])\n",
    "\n",
    "        #first off, we can probably include all player ID's with 100% team penalties taken as 'designated penalty takers'\n",
    "        desig_pen_takers = penprop_summary.loc[penprop_summary['Proportion of Team Penalties Taken'] == 1, 'Player ID'].tolist()\n",
    "\n",
    "        #we now add the Player ID's of players that took more than 50% of their team's penalties \n",
    "        additional_takers = penprop_summary.loc[penprop_summary['Proportion of Team Penalties Taken'] > 0.5, 'Player ID'].tolist()\n",
    "        desig_pen_takers.extend(additional_takers)\n",
    "\n",
    "        #construct 'Designated Penalty Taker' feature \n",
    "        X['Designated Penalty Taker'] = X['Player ID'].isin(desig_pen_takers).astype(int)\n",
    "        \n",
    "        #remove 'Penalties Attempted' column\n",
    "        X = X.drop('Penalties Attempted', axis = 1)\n",
    "    \n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer for calculating 'Rolling xG'\n",
    "class RollingxG(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling xG\n",
    "        def calculate_rolling_xg(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['npxG'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        #apply function\n",
    "        X['Rolling xG'] = X.groupby(['Player ID', 'Season']).apply(calculate_rolling_xg, include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #drop 'npxG' column\n",
    "        X = X.drop('npxG', axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer for calculating 'Rolling Team xG Matchup'\n",
    "class RollingxG_Matchup(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        #load in team data file \n",
    "        team_finaldat = pd.read_csv('team_finaldat.csv', index_col= 0)\n",
    "\n",
    "        #drop irrelevant columns\n",
    "        team_finaldat = team_finaldat.drop(columns = ['Referee', 'Attendance', 'Formation', 'Opposition Formation'])\n",
    "        \n",
    "        #we can see that the team data is first grouped by 'Team', but then 'Date' is backwards. Let's amend this. \n",
    "        team_finaldat = team_finaldat.sort_values(by=['Team', 'Date'], ascending=[True, True])\n",
    "        \n",
    "        #determine_season function, which converts kickoff_time into 'Season\n",
    "        def determine_season(kickoff_time):\n",
    "            month = kickoff_time.month\n",
    "            year = kickoff_time.year\n",
    "            if month >= 8:  # August to December\n",
    "                return f'{year}-{year + 1}'  # Current year to next year\n",
    "            else:  # January to July\n",
    "                return f'{year - 1}-{year}'  # Previous year to current year\n",
    "        \n",
    "        \n",
    "        #we now want to add in rolling xG and xGA for teams. We can try reuse the functions used previously, to do this we need \n",
    "        # to add the 'Season' feature to the team_finaldat dataframe\n",
    "        team_finaldat['Date'] = pd.to_datetime(team_finaldat['Date'])\n",
    "        team_finaldat['Season'] = team_finaldat['Date'].apply(determine_season)\n",
    "\n",
    "        #sort values by Team and Date, we also need to reset the index to ensure that the shifting in the function below works as intended\n",
    "        team_finaldat.sort_values(by=['Team', 'Date'], inplace=True)\n",
    "        team_finaldat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling xG\n",
    "        def calculate_rolling_teamxg(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['xG'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        #function to calculate rolling xGA\n",
    "        def calculate_rolling_teamxga(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['xGA'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        #apply function to get rolling xG and xGA for each team \n",
    "        team_finaldat['Team Rolling xG'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxg, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        team_finaldat['Team Rolling xGA'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxga, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #create xg/xga diff feature \n",
    "        team_finaldat['Team xG Difference'] = team_finaldat['xG'] - team_finaldat['xGA']\n",
    "        \n",
    "        #function to calculate rolling xg diff\n",
    "        def calculate_rolling_teamxgdiff(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['Team xG Difference'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        team_finaldat['Team Rolling xG Difference'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxgdiff, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #merge original dataframe with team data\n",
    "        merged_df = X.merge(\n",
    "            team_finaldat[['Season', 'Venue', 'Team', 'Opponent', 'Team Rolling xG', 'Team Rolling xGA', 'Team Rolling xG Difference']],\n",
    "            on=['Season', 'Venue', 'Team', 'Opponent'],\n",
    "            how='left'  \n",
    "        )\n",
    "\n",
    "        X['Team Rolling xG'] = merged_df['Team Rolling xG']\n",
    "        X['Team Rolling xGA'] = merged_df['Team Rolling xGA']\n",
    "        X['Team Rolling xG Difference'] = merged_df['Team Rolling xG Difference']\n",
    "        \n",
    "        #now, calculate rolling xG team matchups\n",
    "        #create the new feature, lets call it 'Team Rolling xG Matchups' \n",
    "        X['Team Rolling xG Matchup'] = None\n",
    "\n",
    "        #group by 'Team', 'Opponent', 'Season', and 'Venue'\n",
    "        for (team, opponent, season, venue), group in X.groupby(['Team', 'Opponent', 'Season', 'Venue']):\n",
    "            #initialize none values\n",
    "            team_xgdiff = None\n",
    "            opp_xgdiff = None\n",
    "\n",
    "            #get team xg diff\n",
    "            if group['Team Rolling xG Difference'].nunique() == 1:\n",
    "                team_xgdiff = group['Team Rolling xG Difference'].iloc[0]\n",
    "\n",
    "            #get opponent rows\n",
    "            opponent_row = X[\n",
    "                (X['Team'] == opponent) &\n",
    "                (X['Opponent'] == team) &\n",
    "                (X['Season'] == season) &\n",
    "                (X['Venue'] != venue)  # Ensure venue is opposite\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            #make sure opponent rows have the same team rolling xG diff, if so select any\n",
    "            if opponent_row['Team Rolling xG Difference'].nunique() == 1:\n",
    "                opp_xgdiff = opponent_row['Team Rolling xG Difference'].iloc[0]\n",
    "            #calculate xg matchup val\n",
    "            if team_xgdiff is not None and opp_xgdiff is not None:\n",
    "                xg_matchup = team_xgdiff - opp_xgdiff\n",
    "            else:\n",
    "                xg_matchup = None  # handle cases where values are not available\n",
    "\n",
    "            #assign the calculated value back to the original DataFrame\n",
    "            X.loc[group.index, 'Team Rolling xG Matchup'] = xg_matchup\n",
    "        \n",
    "        #drop the unnecessary columns\n",
    "        X = X.drop(['Team Rolling xGA', 'Team Rolling xG Difference'], axis = 1)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    \n",
    "#transformer for calculating 'Rolling Shots on Target'\n",
    "class RollingSOT(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling SOT\n",
    "        def calculate_rolling_sot(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['Shots on Target'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_sot = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_sot\n",
    "\n",
    "        #apply function\n",
    "        X['Rolling Shots on Target'] = X.groupby(['Player ID', 'Season']).apply(calculate_rolling_sot, \n",
    "                                                include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #drop 'Shots on Target' column\n",
    "        X = X.drop('Shots on Target', axis = 1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "#transformer for calculating 'Rolling Penalty Area Touches'\n",
    "class RollingPAT(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling SOT\n",
    "        def calculate_rolling_pat(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['Penalty Area Touches'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_pat = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_pat\n",
    "\n",
    "        #apply function\n",
    "        X['Rolling Penalty Area Touches'] = X.groupby(['Player ID', 'Season']).apply(calculate_rolling_pat, \n",
    "                                                                                     include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #drop 'Penalty Area Touches' column\n",
    "        X = X.drop('Penalty Area Touches', axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "#transformer to converts 'Home' to 0 and 'Away' to 1 in the 'Venue' column\n",
    "class encodeVenue(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        # Ensure 'Venue' column exists in the DataFrame\n",
    "        if 'Venue' in X.columns:\n",
    "            # Replace 'Home' with 0 and 'Away' with 1\n",
    "            X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer that drops the columns in the dataframe that we needed for feature transformation, but we don't need for model fitting\n",
    "class DropFinal(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        X = X.drop(['Player ID', 'Team', 'Opponent', 'kickoff_time', 'Season'], axis = 1)\n",
    "        return X\n",
    "\n",
    "        \n",
    "\n",
    "#FunctionTransformer to select the necessary columns\n",
    "select_transformer = FunctionTransformer(select_columns)\n",
    "\n",
    "# Create a Pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('select', select_transformer), \n",
    "    ('drop_empty_pos', DropEmptyPositions()), \n",
    "    ('encode_positions', PositionEncoder()), \n",
    "    ('determine_season', SeasonDeterminer()), \n",
    "    ('desig_pen_taker', DesigPenTaker()), \n",
    "    ('rolling_xg', RollingxG()), \n",
    "    ('rolling_sot', RollingSOT()),\n",
    "    ('rolling_pat', RollingPAT()),\n",
    "    ('rolling_xg_matchup', RollingxG_Matchup()),\n",
    "    ('encodeVenue', encodeVenue()), \n",
    "    ('drop_final', DropFinal())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_82726/2379564799.py:360: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n",
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_82726/2379564799.py:360: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n"
     ]
    }
   ],
   "source": [
    "#use pipeline to transform the DataFrame\n",
    "att_train_processed = pipe.fit_transform(att_train)\n",
    "att_test_processed = pipe.fit_transform(att_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Venue</th>\n",
       "      <th>Goals</th>\n",
       "      <th>Minutes Played</th>\n",
       "      <th>Defender</th>\n",
       "      <th>Midfielder</th>\n",
       "      <th>Attacker</th>\n",
       "      <th>Designated Penalty Taker</th>\n",
       "      <th>Rolling xG</th>\n",
       "      <th>Rolling Shots on Target</th>\n",
       "      <th>Rolling Penalty Area Touches</th>\n",
       "      <th>Team Rolling xG</th>\n",
       "      <th>Team Rolling xG Matchup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.773897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.604762</td>\n",
       "      <td>0.653209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.803846</td>\n",
       "      <td>1.438462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.837037</td>\n",
       "      <td>0.37599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Venue  Goals  Minutes Played  Defender  Midfielder  Attacker  \\\n",
       "0      1      1              70         0           0         1   \n",
       "1      0      0              72         0           0         1   \n",
       "2      1      1              72         0           0         1   \n",
       "3      1      0              25         0           0         1   \n",
       "4      1      0               6         0           0         1   \n",
       "\n",
       "   Designated Penalty Taker  Rolling xG  Rolling Shots on Target  \\\n",
       "0                         0         NaN                      NaN   \n",
       "1                         0    0.800000                 1.000000   \n",
       "2                         0    0.650000                 1.500000   \n",
       "3                         0    0.766667                 1.333333   \n",
       "4                         0         NaN                      NaN   \n",
       "\n",
       "   Rolling Penalty Area Touches  Team Rolling xG Team Rolling xG Matchup  \n",
       "0                           NaN         1.450000                0.773897  \n",
       "1                      2.000000         1.604762                0.653209  \n",
       "2                      2.500000         1.803846                1.438462  \n",
       "3                      2.666667         1.837037                 0.37599  \n",
       "4                           NaN         2.250000                    0.15  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_test_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the above datasets into feature and target sets\n",
    "\n",
    "att_train_x = att_train_processed.drop('Goals', axis = 1)\n",
    "att_train_y = att_train_processed['Goals']\n",
    "\n",
    "att_test_x = att_test_processed.drop('Goals', axis = 1)\n",
    "att_test_y = att_test_processed['Goals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to begin model training. We will treat this problem as a regression problem, where we want to predict the number of goals scored for a particular player in a particular game as a continuous number (e.g. we may predict x player to score 0.3 goals in x game). As we are treating this as a regression problem, we will use the RMSE (root mean square error) as the score used to evaluate and compare the performance of competing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_base.py\", line 609, in fit\n    X, y = self._validate_data(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      4\u001b[0m linreg_classifier \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m----> 5\u001b[0m linreg_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinreg_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt_train_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matt_train_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneg_root_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display_scores(linreg_scores)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    425\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[1;32m    441\u001b[0m )\n\u001b[0;32m--> 443\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_base.py\", line 609, in fit\n    X, y = self._validate_data(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1301, in check_X_y\n    X = check_array(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_classifier = LinearRegression()\n",
    "linreg_scores = cross_val_score(linreg_classifier, att_train_x, att_train_y, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "display_scores(linreg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
