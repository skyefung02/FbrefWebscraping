{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we build the pipeline to perform the necessary feature transformations prior to model fitting. The necessary changes are listed below. \n",
    "\n",
    "* Removing observations where 'Minutes Played' == 0\n",
    "* Include 'Venue' as important feature\n",
    "* Include 'Minutes Played' as important feature\n",
    "* One-hot encode 'Position', and then further filter into 'Defenders', 'Midfielders' and 'Attackers'\n",
    "* Create 'Season' feature using 'kickoff_time', as this feature is necessary to calculate 'Designated Penalty Takers' \n",
    "* Use 'Penalties Attempted' to calculate 'Designated Penalty Takers' \n",
    "* Include 'Shots on Target' as important feature \n",
    "* Include 'npxG' as important feature\n",
    "* Include 'Penalty Area Touches' as important feature\n",
    "* Compute 'Rolling xG' which will replace 'npxG', then drop 'npxG'\n",
    "* Calculate 'Team Rolling xG Matchups' by first using team data to first calculate 'Team Rolling xG' and 'Team Rolling xGA'. We can then calculate 'Team Rolling xG Difference', which allows us to calculate 'Team Rolling xG Matchups'\n",
    "* Calculate 'Rolling Shots on Target' to replace 'Shots on Target', then drop 'Shots on Target'\n",
    "* Calculate 'Rolling Penalty Area Touches' to replace 'Penalty Area Touches', then drop 'Penalty Area Touches'\n",
    "\n",
    "This should leave the final dataframe with the following features - Venue, Designated Penalty Taker, Rolling Shots on Target, Rolling xG, Rolling Penalty Area Touches, Team Rolling xG Matchup, Defenders, Midfielders, Attackers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant dataframes from source\n",
    "att_train = pd.read_csv('att_explore_original.csv', index_col = 0)\n",
    "att_test = pd.read_csv('att_test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to select specific columns\n",
    "def select_columns(dataframe):\n",
    "    columns = ['Player ID', 'Team', 'Opponent', 'Venue', 'Goals', 'Minutes Played', 'Position', 'kickoff_time', 'Penalties Attempted', \n",
    "               'Shots on Target', 'npxG', 'Penalty Area Touches'] \n",
    "    return dataframe[columns].copy()\n",
    "\n",
    "#transformer to drop rows with empty 'Position'\n",
    "class DropEmptyPositions(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[X['Position'] != '0']\n",
    "\n",
    "#transformer for one-hot encoding and creating position categories\n",
    "class PositionEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #one hot encode positions\n",
    "        positions_encode = X['Position'].str.get_dummies(sep=',')\n",
    "        \n",
    "        #create new position categories\n",
    "        positions_encode['Defender'] = positions_encode[['RB', 'LB', 'CB']].any(axis=1).astype(int)\n",
    "        positions_encode['Midfielder'] = positions_encode[['DM', 'CM', 'LM', 'RM', 'AM']].any(axis=1).astype(int)\n",
    "        positions_encode['Attacker'] = positions_encode[['LW', 'RW', 'FW']].any(axis=1).astype(int)\n",
    "        \n",
    "        # Drop original position columns\n",
    "        positions_encode = positions_encode.drop(columns=['RB', 'LB', 'CB', 'DM', 'CM', 'LM', 'RM', 'LW', 'RW', 'AM', 'FW', 'WB'], \n",
    "                                                 errors='ignore')\n",
    "        \n",
    "        X = X.drop('Position', axis = 1)\n",
    "        \n",
    "        return pd.concat([X.reset_index(drop=True), positions_encode.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#transformer for determining the season\n",
    "class SeasonDeterminer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert 'kickoff_time' to datetime if not already done\n",
    "        X['kickoff_time'] = pd.to_datetime(X['kickoff_time'])\n",
    "        \n",
    "        # Function to determine the season\n",
    "        def determine_season(kickoff_time):\n",
    "            month = kickoff_time.month\n",
    "            year = kickoff_time.year\n",
    "            if month >= 8:  # August to December\n",
    "                return f'{year}-{year + 1}'  # Current year to next year\n",
    "            else:  # January to July\n",
    "                return f'{year - 1}-{year}'  # Previous year to current year\n",
    "        \n",
    "        # Apply the function to create the 'Season' column\n",
    "        X['Season'] = X['kickoff_time'].apply(determine_season)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "#transformer for calculating 'Designated Penalty Taker'\n",
    "class DesigPenTaker(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #group observations by player ID and penalties attempted \n",
    "        pen_group = X.groupby('Player ID', as_index = False)['Penalties Attempted'].sum()\n",
    "\n",
    "        #remove obs with 0 penalties attempted \n",
    "        pen_group = pen_group[pen_group['Penalties Attempted'] > 0 ]\n",
    "\n",
    "        #create new dataframe which has 'kickoff_time', 'Season', 'penalties attempted' and 'team in it \n",
    "        team_pens = X[['kickoff_time', 'Season', 'Penalties Attempted', 'Team']].copy()\n",
    "\n",
    "        #now we group by team and season to compute how many penalties were taken by each team in each season\n",
    "        team_pens_summary = team_pens.groupby(['Season', 'Team'], as_index=False)['Penalties Attempted'].sum()\n",
    "        team_pens_summary.rename(columns={'Penalties Attempted': 'Team Penalties'}, inplace=True)\n",
    "\n",
    "        #create empty dataframe\n",
    "        pen_prop = pd.DataFrame()\n",
    "\n",
    "        #loop through to get the Player ID and Penalties Attempted for each team in each season, filtering so that we only \n",
    "        # include observations with at least 1 penalty taken \n",
    "        for index, row in team_pens_summary.iterrows():\n",
    "            team = row['Team']\n",
    "            season = row['Season']\n",
    "    \n",
    "            filtered = X[(X['Season'] == season) & (X['Team'] == team) & (X['Penalties Attempted'] > 0)][['Player ID', 'Penalties Attempted']]\n",
    "            filtered['Team'] = team\n",
    "            filtered['Season'] = season\n",
    "            pen_prop = pd.concat([pen_prop, filtered], ignore_index= True)\n",
    "\n",
    "        #adding a new column into pen_prop called 'Team Penalties' which merges the relevant information from team_pens_summary\n",
    "        pen_prop = pen_prop.merge(team_pens_summary, on=['Team', 'Season'], how='left')\n",
    "\n",
    "        #we now merge rows that have the same player ID, team and season together. For the rows that satisfy this, we sum the \n",
    "        # penalties attempted to reflect the number of penalties a particular player ID took in a given season \n",
    "        merged_penprop = pen_prop.groupby(['Team', 'Season', 'Player ID'], as_index=False).agg({\n",
    "            'Penalties Attempted': 'sum',\n",
    "            'Team Penalties': 'first'  \n",
    "        })\n",
    "        merged_penprop = merged_penprop.sort_values(by='Player ID')\n",
    "\n",
    "        #adding new column called Proportion of Team Penalties Taken\n",
    "        merged_penprop['Proportion of Team Penalties Taken'] = (merged_penprop['Penalties Attempted'] / merged_penprop['Team Penalties'])\n",
    "\n",
    "        #final dataframe which merges the rows based on Player ID. Each row now corresponds to one unique player ID, the penalties\n",
    "        # attempted and team penalties columns are now summed. The proportion is then recalculated \n",
    "        penprop_summary = merged_penprop.groupby('Player ID').agg(\n",
    "            Penalties_Attempted=('Penalties Attempted', 'sum'),\n",
    "            Team_Penalties=('Team Penalties', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        penprop_summary['Proportion of Team Penalties Taken'] = (\n",
    "        penprop_summary['Penalties_Attempted'] / penprop_summary['Team_Penalties'])\n",
    "\n",
    "        #first off, we can probably include all player ID's with 100% team penalties taken as 'designated penalty takers'\n",
    "        desig_pen_takers = penprop_summary.loc[penprop_summary['Proportion of Team Penalties Taken'] == 1, 'Player ID'].tolist()\n",
    "\n",
    "        #we now add the Player ID's of players that took more than 50% of their team's penalties \n",
    "        additional_takers = penprop_summary.loc[penprop_summary['Proportion of Team Penalties Taken'] > 0.5, 'Player ID'].tolist()\n",
    "        desig_pen_takers.extend(additional_takers)\n",
    "\n",
    "        #construct 'Designated Penalty Taker' feature \n",
    "        X['Designated Penalty Taker'] = X['Player ID'].isin(desig_pen_takers).astype(int)\n",
    "        \n",
    "        #remove 'Penalties Attempted' column\n",
    "        X = X.drop('Penalties Attempted', axis = 1)\n",
    "    \n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer for calculating 'Rolling xG'\n",
    "class RollingxG(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling xG (past 365 days version)\n",
    "        def calculate_rolling_xg(row, df):\n",
    "            player_id = row['Player ID']\n",
    "            kickoff_time = row['kickoff_time']\n",
    "    \n",
    "            # Define the date range\n",
    "            start_date = kickoff_time - pd.Timedelta(days=365)\n",
    "            end_date = kickoff_time - pd.Timedelta(days=1)  # exclusive of the kickoff_time\n",
    "    \n",
    "            # Filter the DataFrame for the specific player and date range\n",
    "            player_data = df[(df['Player ID'] == player_id) & \n",
    "                            (df['kickoff_time'] >= start_date) & \n",
    "                            (df['kickoff_time'] <= end_date)]\n",
    "    \n",
    "            # Calculate total xG and number of games\n",
    "            total_xG = player_data['npxG'].sum()\n",
    "            number_of_games = player_data.shape[0]\n",
    "    \n",
    "            # Calculate average xG per game\n",
    "            if number_of_games > 0:\n",
    "                return total_xG / number_of_games\n",
    "            else:\n",
    "                return None  # No games played in the period\n",
    "    \n",
    "\n",
    "        # Apply the function to create the 'rolling xG' column\n",
    "        X.loc[:, 'Rolling xG'] = X.apply(lambda row: calculate_rolling_xg(row, X), axis=1)\n",
    "        \n",
    "        #drop 'npxG' column\n",
    "        X = X.drop('npxG', axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer for calculating 'Rolling Team xG Matchup'\n",
    "class RollingxG_Matchup(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        #load in team data file \n",
    "        team_finaldat = pd.read_csv('team_finaldat.csv', index_col= 0)\n",
    "\n",
    "        #drop irrelevant columns\n",
    "        team_finaldat = team_finaldat.drop(columns = ['Referee', 'Attendance', 'Formation', 'Opposition Formation'])\n",
    "        \n",
    "        #we can see that the team data is first grouped by 'Team', but then 'Date' is backwards. Let's amend this. \n",
    "        team_finaldat = team_finaldat.sort_values(by=['Team', 'Date'], ascending=[True, True])\n",
    "        \n",
    "        #determine_season function, which converts kickoff_time into 'Season\n",
    "        def determine_season(kickoff_time):\n",
    "            month = kickoff_time.month\n",
    "            year = kickoff_time.year\n",
    "            if month >= 8:  # August to December\n",
    "                return f'{year}-{year + 1}'  # Current year to next year\n",
    "            else:  # January to July\n",
    "                return f'{year - 1}-{year}'  # Previous year to current year\n",
    "        \n",
    "        \n",
    "        #we now want to add in rolling xG and xGA for teams. We can try reuse the functions used previously, to do this we need \n",
    "        # to add the 'Season' feature to the team_finaldat dataframe\n",
    "        team_finaldat['Date'] = pd.to_datetime(team_finaldat['Date'])\n",
    "        team_finaldat['Season'] = team_finaldat['Date'].apply(determine_season)\n",
    "\n",
    "        #sort values by Team and Date, we also need to reset the index to ensure that the shifting in the function below works as intended\n",
    "        team_finaldat.sort_values(by=['Team', 'Date'], inplace=True)\n",
    "        team_finaldat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling xG\n",
    "        def calculate_rolling_teamxg(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['xG'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        #function to calculate rolling xGA\n",
    "        def calculate_rolling_teamxga(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['xGA'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        #apply function to get rolling xG and xGA for each team \n",
    "        team_finaldat['Team Rolling xG'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxg, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        team_finaldat['Team Rolling xGA'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxga, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #create xg/xga diff feature \n",
    "        team_finaldat['Team xG Difference'] = team_finaldat['xG'] - team_finaldat['xGA']\n",
    "        \n",
    "        #function to calculate rolling xg diff\n",
    "        def calculate_rolling_teamxgdiff(group):\n",
    "            # Calculate the cumulative sum and the number of games played\n",
    "            cumulative_sum = group['Team xG Difference'].cumsum()\n",
    "            count = pd.Series(range(1, len(group) + 1), index=group.index)\n",
    "    \n",
    "            # Create a new Series for rolling xG\n",
    "            rolling_xg = cumulative_sum.shift(1)/count.shift(1)\n",
    "    \n",
    "            return rolling_xg\n",
    "\n",
    "        team_finaldat['Team Rolling xG Difference'] = team_finaldat.groupby(['Team', 'Season']).apply(calculate_rolling_teamxgdiff, \n",
    "                                                                                include_groups = False).reset_index(drop = True)\n",
    "        \n",
    "        #merge original dataframe with team data\n",
    "        merged_df = X.merge(\n",
    "            team_finaldat[['Season', 'Venue', 'Team', 'Opponent', 'Team Rolling xG', 'Team Rolling xGA', 'Team Rolling xG Difference']],\n",
    "            on=['Season', 'Venue', 'Team', 'Opponent'],\n",
    "            how='left'  \n",
    "        )\n",
    "\n",
    "        X['Team Rolling xG'] = merged_df['Team Rolling xG']\n",
    "        X['Team Rolling xGA'] = merged_df['Team Rolling xGA']\n",
    "        X['Team Rolling xG Difference'] = merged_df['Team Rolling xG Difference']\n",
    "        \n",
    "        #now, calculate rolling xG team matchups\n",
    "        #create the new feature, lets call it 'Team Rolling xG Matchups' \n",
    "        X['Team Rolling xG Matchup'] = None\n",
    "\n",
    "        #group by 'Team', 'Opponent', 'Season', and 'Venue'\n",
    "        for (team, opponent, season, venue), group in X.groupby(['Team', 'Opponent', 'Season', 'Venue']):\n",
    "            #initialize none values\n",
    "            team_xgdiff = None\n",
    "            opp_xgdiff = None\n",
    "\n",
    "            #get team xg diff\n",
    "            if group['Team Rolling xG Difference'].nunique() == 1:\n",
    "                team_xgdiff = group['Team Rolling xG Difference'].iloc[0]\n",
    "\n",
    "            #get opponent rows\n",
    "            opponent_row = X[\n",
    "                (X['Team'] == opponent) &\n",
    "                (X['Opponent'] == team) &\n",
    "                (X['Season'] == season) &\n",
    "                (X['Venue'] != venue)  # Ensure venue is opposite\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "            #make sure opponent rows have the same team rolling xG diff, if so select any\n",
    "            if opponent_row['Team Rolling xG Difference'].nunique() == 1:\n",
    "                opp_xgdiff = opponent_row['Team Rolling xG Difference'].iloc[0]\n",
    "            #calculate xg matchup val\n",
    "            if team_xgdiff is not None and opp_xgdiff is not None:\n",
    "                xg_matchup = team_xgdiff - opp_xgdiff\n",
    "            else:\n",
    "                xg_matchup = None  # handle cases where values are not available\n",
    "\n",
    "            #assign the calculated value back to the original DataFrame\n",
    "            X.loc[group.index, 'Team Rolling xG Matchup'] = xg_matchup\n",
    "        \n",
    "        \n",
    "        #convert values in 'Team Rolling xG Matchup' from object to float\n",
    "        X['Team Rolling xG Matchup'] = X['Team Rolling xG Matchup'].astype(float)\n",
    "        \n",
    "        #drop the unnecessary columns\n",
    "        X = X.drop(['Team Rolling xGA', 'Team Rolling xG Difference'], axis = 1)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    \n",
    "#transformer for calculating 'Rolling Shots on Target'\n",
    "class RollingSOT(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #function to calculate rolling xG (past 365 days version)\n",
    "        def calculate_rolling_sot(row, df):\n",
    "            player_id = row['Player ID']\n",
    "            kickoff_time = row['kickoff_time']\n",
    "    \n",
    "            # Define the date range\n",
    "            start_date = kickoff_time - pd.Timedelta(days=365)\n",
    "            end_date = kickoff_time - pd.Timedelta(days=1)  # exclusive of the kickoff_time\n",
    "    \n",
    "            # Filter the DataFrame for the specific player and date range\n",
    "            player_data = df[(df['Player ID'] == player_id) & \n",
    "                            (df['kickoff_time'] >= start_date) & \n",
    "                            (df['kickoff_time'] <= end_date)]\n",
    "    \n",
    "            # Calculate total xG and number of games\n",
    "            total_xG = player_data['Shots on Target'].sum()\n",
    "            number_of_games = player_data.shape[0]\n",
    "    \n",
    "            # Calculate average xG per game\n",
    "            if number_of_games > 0:\n",
    "                return total_xG / number_of_games\n",
    "            else:\n",
    "                return None  # No games played in the period\n",
    "    \n",
    "\n",
    "        # Apply the function to create the 'rolling xG' column\n",
    "        X.loc[:, 'Rolling Shots on Target'] = X.apply(lambda row: calculate_rolling_sot(row, X), axis=1)\n",
    "        \n",
    "        #drop 'Shots on Target' column\n",
    "        X = X.drop('Shots on Target', axis = 1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "#transformer for calculating 'Rolling Penalty Area Touches'\n",
    "class RollingPAT(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        #sort values by Player ID and kickoff_time, we also need to reset the index to ensure that the shifting in the function \n",
    "        # below works as intended\n",
    "        X.sort_values(by=['Player ID', 'kickoff_time'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "       #function to calculate rolling xG (past 365 days version)\n",
    "        def calculate_rolling_pat(row, df):\n",
    "            player_id = row['Player ID']\n",
    "            kickoff_time = row['kickoff_time']\n",
    "    \n",
    "            # Define the date range\n",
    "            start_date = kickoff_time - pd.Timedelta(days=365)\n",
    "            end_date = kickoff_time - pd.Timedelta(days=1)  # exclusive of the kickoff_time\n",
    "    \n",
    "            # Filter the DataFrame for the specific player and date range\n",
    "            player_data = df[(df['Player ID'] == player_id) & \n",
    "                            (df['kickoff_time'] >= start_date) & \n",
    "                            (df['kickoff_time'] <= end_date)]\n",
    "    \n",
    "            # Calculate total xG and number of games\n",
    "            total_xG = player_data['Penalty Area Touches'].sum()\n",
    "            number_of_games = player_data.shape[0]\n",
    "    \n",
    "            # Calculate average xG per game\n",
    "            if number_of_games > 0:\n",
    "                return total_xG / number_of_games\n",
    "            else:\n",
    "                return None  # No games played in the period\n",
    "    \n",
    "\n",
    "        # Apply the function to create the 'rolling xG' column\n",
    "        X.loc[:, 'Rolling Penalty Area Touches'] = X.apply(lambda row: calculate_rolling_pat(row, X), axis=1)\n",
    "        \n",
    "        #drop 'Penalty Area Touches' column\n",
    "        X = X.drop('Penalty Area Touches', axis = 1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "#transformer to converts 'Home' to 0 and 'Away' to 1 in the 'Venue' column\n",
    "class encodeVenue(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        # Ensure 'Venue' column exists in the DataFrame\n",
    "        if 'Venue' in X.columns:\n",
    "            # Replace 'Home' with 0 and 'Away' with 1\n",
    "            X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n",
    "        return X\n",
    "\n",
    "\n",
    "#transformer that drops the columns in the dataframe that we needed for feature transformation, but we don't need for model fitting\n",
    "class DropFinal(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        X = X.drop(['Player ID', 'Team', 'Opponent', 'kickoff_time', 'Season'], axis = 1)\n",
    "        return X\n",
    "    \n",
    "\n",
    "#transformer that drops the rows that have NaNs\n",
    "class DropRow(BaseEstimator, TransformerMixin):\n",
    "    def fit (self, X, y = None):\n",
    "        return self \n",
    "    def transform(self, X):\n",
    "        X = X.dropna()\n",
    "        return X\n",
    "\n",
    "        \n",
    "\n",
    "#FunctionTransformer to select the necessary columns\n",
    "select_transformer = FunctionTransformer(select_columns)\n",
    "\n",
    "# Create a Pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('select', select_transformer), \n",
    "    ('drop_empty_pos', DropEmptyPositions()), \n",
    "    ('encode_positions', PositionEncoder()), \n",
    "    ('determine_season', SeasonDeterminer()), \n",
    "    ('desig_pen_taker', DesigPenTaker()), \n",
    "    ('rolling_xg', RollingxG()), \n",
    "    ('rolling_sot', RollingSOT()),\n",
    "    ('rolling_pat', RollingPAT()),\n",
    "    ('rolling_xg_matchup', RollingxG_Matchup()),\n",
    "    ('encodeVenue', encodeVenue()), \n",
    "    ('drop_final', DropFinal()), \n",
    "    ('droprow', DropRow())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_82726/2556830359.py:404: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n",
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_82726/2556830359.py:404: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X['Venue'] = X['Venue'].replace({'Home': 0, 'Away': 1})\n"
     ]
    }
   ],
   "source": [
    "#use pipeline to transform the DataFrame\n",
    "att_train_processed = pipe.fit_transform(att_train)\n",
    "att_test_processed = pipe.fit_transform(att_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Venue</th>\n",
       "      <th>Goals</th>\n",
       "      <th>Minutes Played</th>\n",
       "      <th>Defender</th>\n",
       "      <th>Midfielder</th>\n",
       "      <th>Attacker</th>\n",
       "      <th>Designated Penalty Taker</th>\n",
       "      <th>Rolling xG</th>\n",
       "      <th>Rolling Shots on Target</th>\n",
       "      <th>Rolling Penalty Area Touches</th>\n",
       "      <th>Team Rolling xG</th>\n",
       "      <th>Team Rolling xG Matchup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.604762</td>\n",
       "      <td>0.653209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.803846</td>\n",
       "      <td>1.438462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.837037</td>\n",
       "      <td>0.375990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.663636</td>\n",
       "      <td>0.413636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Venue  Goals  Minutes Played  Defender  Midfielder  Attacker  \\\n",
       "1      0      0              72         0           0         1   \n",
       "2      1      1              72         0           0         1   \n",
       "3      1      0              25         0           0         1   \n",
       "4      1      0               6         0           0         1   \n",
       "6      0      0              68         0           0         1   \n",
       "\n",
       "   Designated Penalty Taker  Rolling xG  Rolling Shots on Target  \\\n",
       "1                         0    0.800000                 1.000000   \n",
       "2                         0    0.650000                 1.500000   \n",
       "3                         0    0.766667                 1.333333   \n",
       "4                         0    0.600000                 1.000000   \n",
       "6                         0    0.360000                 1.000000   \n",
       "\n",
       "   Rolling Penalty Area Touches  Team Rolling xG  Team Rolling xG Matchup  \n",
       "1                      2.000000         1.604762                 0.653209  \n",
       "2                      2.500000         1.803846                 1.438462  \n",
       "3                      2.666667         1.837037                 0.375990  \n",
       "4                      2.250000         2.250000                 0.150000  \n",
       "6                      2.200000         1.663636                 0.413636  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_test_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the above datasets into feature and target sets\n",
    "\n",
    "att_train_x = att_train_processed.drop('Goals', axis = 1)\n",
    "att_train_y = att_train_processed['Goals']\n",
    "\n",
    "att_test_x = att_test_processed.drop('Goals', axis = 1)\n",
    "att_test_y = att_test_processed['Goals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to begin model training. We will treat this problem as a regression problem, where we want to predict the number of goals scored for a particular player in a particular game as a continuous number (e.g. we may predict x player to score 0.3 goals in x game). As we are treating this as a regression problem, we will use the RMSE (root mean square error) as the score used to evaluate and compare the performance of competing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-0.38641969 -0.36030985 -0.38296061 -0.38685603 -0.39624085 -0.39368591\n",
      " -0.3746901  -0.44459941 -0.40338992 -0.39929472]\n",
      "Mean: -0.39284470866088456\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_regressor = LinearRegression()\n",
    "linreg_scores = cross_val_score(linreg_regressor, att_train_x, att_train_y, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "display_scores(linreg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-0.39951069 -0.36957843 -0.3935038  -0.39438577 -0.40486752 -0.398403\n",
      " -0.379545   -0.45686912 -0.41121226 -0.40952668]\n",
      "Mean: -0.4017402256578334\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=66)\n",
    "rf_regressor.fit(att_train_x, att_train_y)\n",
    "\n",
    "rf_scores = cross_val_score(rf_regressor, att_train_x, att_train_y, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "display_scores(rf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-0.40151524 -0.3730359  -0.39291891 -0.400093   -0.40529737 -0.41148722\n",
      " -0.38312648 -0.47087469 -0.41777239 -0.41218141]\n",
      "Mean: -0.4068302604474381\n"
     ]
    }
   ],
   "source": [
    "#xgBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xg_regressor = XGBRegressor(random_state=66)\n",
    "xg_scores = cross_val_score(xg_regressor, att_train_x, att_train_y, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "display_scores(xg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initial exploration, it appears as if all 3 models perform comparatively in terms of predictive performance. The average 'negative root mean square error' is basically the same for all the models being considered. \n",
    "\n",
    "Let's try move forward with optimising the random forest model, to see if we can achieve better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 300}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "rfgrid_search = GridSearchCV(rf_regressor, param_grid, cv = 10, scoring = \"neg_root_mean_squared_error\", return_train_score= True, verbose = 1)\n",
    "rfgrid_search.fit(att_train_x, att_train_y)\n",
    "rfgrid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the grid search from above, the parameters returned are\n",
    "* max_depth - 10 \n",
    "* min_samples_split - 2 \n",
    "* n_estimators- 300\n",
    "\n",
    "As the n_estimators hyperparameter is at an upper bound, we can run another grid search with max_depth and min_samples_split set at 10 and 2 respectively, but increasing the upper bound of n_estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 400}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_extend = {\n",
    "    'n_estimators': [300, 400, 500],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "rfgrid_search = GridSearchCV(rf_regressor, param_grid_extend, cv = 10, scoring = \"neg_root_mean_squared_error\", return_train_score= True, verbose = 1)\n",
    "rfgrid_search.fit(att_train_x, att_train_y)\n",
    "rfgrid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new parameter grid, we are now getting the following hyperparameters \n",
    "* n_estimators = 400\n",
    "* max_depth = 10\n",
    "* min_samples_split = 2\n",
    "Let's try fit another random forest with these hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [-0.38878226 -0.35929697 -0.38416757 -0.3852862  -0.39586932 -0.39294758\n",
      " -0.37032759 -0.44662431 -0.40252474 -0.3998467 ]\n",
      "Mean: -0.39256732386190857\n"
     ]
    }
   ],
   "source": [
    "rf_regressor = RandomForestRegressor(n_estimators=400, max_depth = 10, min_samples_split = 2, random_state=66)\n",
    "rf_regressor.fit(att_train_x, att_train_y)\n",
    "\n",
    "rf_scores = cross_val_score(rf_regressor, att_train_x, att_train_y, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "display_scores(rf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the performance of this updated model has improved slightly. However, the improvement is quite insignificant. Let's now try to perform hyperparamter optimisation using Optuna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 12:30:56,687] A new study created in memory with name: no-name-02d672e1-eaae-4573-ba8e-7e863fccdee4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810f3e2ff0644992adfc8f36bf017945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-12-17 12:39:12,398] Trial 0 finished with value: -0.3994068242861052 and parameters: {'n_estimators': 450, 'max_depth': 21, 'min_samples_split': 4}. Best is trial 0 with value: -0.3994068242861052.\n",
      "[I 2024-12-17 12:43:51,445] Trial 1 finished with value: -0.39337464672881695 and parameters: {'n_estimators': 500, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 1 with value: -0.39337464672881695.\n",
      "[I 2024-12-17 12:46:13,616] Trial 2 finished with value: -0.3910432160164683 and parameters: {'n_estimators': 450, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 12:51:24,807] Trial 3 finished with value: -0.3999035157468507 and parameters: {'n_estimators': 400, 'max_depth': 21, 'min_samples_split': 2}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 12:57:24,676] Trial 4 finished with value: -0.39717990474234754 and parameters: {'n_estimators': 500, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:03:53,967] Trial 5 finished with value: -0.39926152645212604 and parameters: {'n_estimators': 500, 'max_depth': 21, 'min_samples_split': 5}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:09:45,304] Trial 6 finished with value: -0.399832399002177 and parameters: {'n_estimators': 450, 'max_depth': 21, 'min_samples_split': 2}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:11:19,666] Trial 7 finished with value: -0.391053491740404 and parameters: {'n_estimators': 300, 'max_depth': 6, 'min_samples_split': 5}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:16:01,814] Trial 8 finished with value: -0.39337464672881695 and parameters: {'n_estimators': 500, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:19:38,809] Trial 9 finished with value: -0.3972528580646181 and parameters: {'n_estimators': 300, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:20:02,190] Trial 10 finished with value: -0.40285875386779973 and parameters: {'n_estimators': 400, 'max_depth': 1, 'min_samples_split': 3}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:20:19,947] Trial 11 finished with value: -0.40286389362491304 and parameters: {'n_estimators': 300, 'max_depth': 1, 'min_samples_split': 5}. Best is trial 2 with value: -0.3910432160164683.\n",
      "[I 2024-12-17 13:22:11,551] Trial 12 finished with value: -0.39103957313350124 and parameters: {'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:24:01,680] Trial 13 finished with value: -0.39103957313350124 and parameters: {'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:25:52,272] Trial 14 finished with value: -0.39103957313350124 and parameters: {'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:27:42,961] Trial 15 finished with value: -0.39103957313350124 and parameters: {'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:28:02,652] Trial 16 finished with value: -0.4028616796754866 and parameters: {'n_estimators': 350, 'max_depth': 1, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:29:52,809] Trial 17 finished with value: -0.39104222990056475 and parameters: {'n_estimators': 350, 'max_depth': 6, 'min_samples_split': 4}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:33:35,502] Trial 18 finished with value: -0.3933666068611704 and parameters: {'n_estimators': 400, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 12 with value: -0.39103957313350124.\n",
      "[I 2024-12-17 13:36:50,913] Trial 19 finished with value: -0.3933860650796198 and parameters: {'n_estimators': 350, 'max_depth': 11, 'min_samples_split': 4}. Best is trial 12 with value: -0.39103957313350124.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', low = 300, high = 500, step = 50)\n",
    "    max_depth = trial.suggest_int('max_depth', low = 1, high = 21, step = 5)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 5)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=66)\n",
    "    \n",
    "    scores = cross_val_score(model, att_train_x, att_train_y, cv = 10, scoring = \"neg_root_mean_squared_error\")\n",
    "    return(scores.mean())\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials = 20, show_progress_bar = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
